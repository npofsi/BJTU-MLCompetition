{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3362a434",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2666d6fa3accbbdd\n",
      "Reusing dataset csv (/home/zhangj/.cache/huggingface/datasets/csv/default-2666d6fa3accbbdd/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
      "Using custom data configuration default-2666d6fa3accbbdd\n",
      "Reusing dataset csv (/home/zhangj/.cache/huggingface/datasets/csv/default-2666d6fa3accbbdd/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
      "Using custom data configuration default-2666d6fa3accbbdd\n",
      "Reusing dataset csv (/home/zhangj/.cache/huggingface/datasets/csv/default-2666d6fa3accbbdd/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 1500, 5000, ('广州首个互联网产业园预计10月挂牌', 0))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "BASE = os.getcwd()  #项目目录\n",
    "\n",
    "#定义数据集\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        self.label2id = {\"科技\":0,\"股票\":1,\"教育\":2,\"财经\":3,\"娱乐\":4} \n",
    "        #存放训练集 验证集和测试集的路径\n",
    "        data_files={\n",
    "            \"train\": [f\"{BASE}/data/extract/{label}-train.csv\" for label in  self.label2id],\n",
    "            \"dev\": [f\"{BASE}/data/extract/{label}-dev.csv\" for label in  self.label2id],\n",
    "            \"test\": [f\"{BASE}/data/extract/{label}-test.csv\" for label in  self.label2id],\n",
    "        } \n",
    "        #读取数据，delimiter是每行的分隔符，column_names是文件数据的列名\n",
    "        self.dataset = load_dataset('csv', data_files=data_files, delimiter='\\t', column_names=[ \"label\",\"title\", \"content\"], split=split)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i): #迭代生成每条数据\n",
    "        text = self.dataset[i]['title'] #一个i就是一条数据，将标题作为训练的文本，也可以增加content\n",
    "        label = self.label2id[self.dataset[i]['label']]  #把文字的label转换成id\n",
    "\n",
    "        return text, label\n",
    "\n",
    "\n",
    "train_dataset = Dataset('train')\n",
    "dev_dataset = Dataset('dev')\n",
    "test_dataset = Dataset('test')\n",
    "\n",
    "len(train_dataset),len(dev_dataset),len(test_dataset), train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e70a58c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='/data/zhangj/bert-base/model/bert-base-chinese', vocab_size=21128, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "#加载字典和分词工具\n",
    "tokenizer = BertTokenizer.from_pretrained(f'{BASE}/model/bert-base-chinese')\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e59695a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 50]),\n",
       " torch.Size([32, 50]),\n",
       " torch.Size([32, 50]),\n",
       " tensor([0, 1, 0, 3, 0, 0, 4, 4, 3, 0, 2, 2, 0, 3, 2, 0, 2, 4, 1, 4, 2, 2, 3, 4,\n",
       "         1, 3, 2, 1, 4, 4, 2, 2]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn(data): #数据加载函数\n",
    "    # print(data)\n",
    "    sents = [i[0] for i in data]\n",
    "    labels = [i[1] for i in data]\n",
    "\n",
    "    #编码，把中文变成id\n",
    "    data = tokenizer.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                    #当句子长度大于max_length时,截断\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length', #一律补零到max_length长度\n",
    "                                   max_length=50,\n",
    "                                   return_tensors='pt', #可取值tf,pt,np,默认为返回list\n",
    "                                   return_length=True)  #返回length 标识长度\n",
    "\n",
    "    #input_ids:编码之后的数字\n",
    "    input_ids = data['input_ids']\n",
    "    #attention_mask:是补零的位置是0,其他位置是1\n",
    "    attention_mask = data['attention_mask']\n",
    "    #token_type_ids 第一个句子和特殊符号的位置是0,第二个句子的位置是1\n",
    "    token_type_ids = data['token_type_ids']\n",
    "\n",
    "    #标签\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    #print(data['length'], data['length'].max())\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "\n",
    "#数据加载器\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                     batch_size=32,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "dev_loader = torch.utils.data.DataLoader(dataset=dev_dataset,\n",
    "                                     batch_size=32,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                     batch_size=32,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(train_loader):\n",
    "    break\n",
    "\n",
    "print(len(train_loader))\n",
    "input_ids.shape, attention_mask.shape, token_type_ids.shape, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f620d0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "import os\n",
    "\n",
    "device = 'cpu' \n",
    "device = 'cuda:0' #如果有GPU可以换成'cuda:0'\n",
    "\n",
    "#加载预训练模型Bert\n",
    "pretrained = BertModel.from_pretrained(f'{BASE}/model/bert-base-chinese').to(device)\n",
    "\n",
    "#不训练,不需要计算梯度\n",
    "# for param in pretrained.parameters():\n",
    "#     param.requires_grad_(False)\n",
    "\n",
    "#模型试算\n",
    "out = pretrained(input_ids=input_ids.to(device),\n",
    "           attention_mask=attention_mask.to(device),\n",
    "           token_type_ids=token_type_ids.to(device))\n",
    "\n",
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d3d02a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义下游任务模型\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support,f1_score\n",
    "import math\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self,num_label,bert):\n",
    "        super().__init__()\n",
    "        #定义分类器\n",
    "        self.fc = torch.nn.Linear(768, num_label)\n",
    "        #定义bert\n",
    "        self.bert = bert\n",
    "\n",
    "    #指标计算函数\n",
    "    def compute(self,labels, preds):\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
    "        if precision + recall < 0.01:\n",
    "            precision = 0.01\n",
    "        # f1 = 2 * precision * recall / (precision + recall)\n",
    "        if math.isnan(f1):\n",
    "            f1 = 0\n",
    "            precision = 0.01\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\n",
    "            'accuracy': acc,\n",
    "            'f1': f1,\n",
    "            'precision': precision,\n",
    "            'recall': recall\n",
    "        }\n",
    "    \n",
    "    #前向传播过程\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            #得到bert最后一层的每句话的输出向量，shape=(bs,maxlen,768)\n",
    "            out = self.bert(input_ids=input_ids,\n",
    "                       attention_mask=attention_mask,\n",
    "                       token_type_ids=token_type_ids)\n",
    "\n",
    "        out = self.fc(out.last_hidden_state[:, 0]) #取出[CLS]的首部信息作为分类器的输入\n",
    "\n",
    "        out = out.softmax(dim=1) #softmax转换成概率\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model(5,pretrained).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bd44a7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** epoch1,batch1 train loss 1.2083163261413574, dev loss 1.1821329593658447, dev f1 0.8621039983483978, best f1-macro now:0\n",
      "*** epoch1,batch51 train loss 1.2121587991714478, dev loss 1.1329888105392456, dev f1 0.8632374712723792, best f1-macro now:0.8621039983483978\n",
      "*** epoch1,batch101 train loss 1.1789382696151733, dev loss 1.1485633850097656, dev f1 0.8656054573271567, best f1-macro now:0.8632374712723792\n",
      "*** epoch1,batch151 train loss 1.1174002885818481, dev loss 1.1126203536987305, dev f1 0.869636326572231, best f1-macro now:0.8656054573271567\n",
      "*** epoch1,batch201 train loss 1.1982020139694214, dev loss 1.1299853324890137, dev f1 0.8679107751590293, best f1-macro now:0.869636326572231\n",
      "*** epoch1,batch251 train loss 1.1362107992172241, dev loss 1.0824124813079834, dev f1 0.868089728314757, best f1-macro now:0.869636326572231\n",
      "*** epoch1,batch301 train loss 1.0984047651290894, dev loss 1.1879661083221436, dev f1 0.8693344588765655, best f1-macro now:0.869636326572231\n",
      "*** epoch2,batch1 train loss 1.11082923412323, dev loss 1.0803815126419067, dev f1 0.8725668663758895, best f1-macro now:0.869636326572231\n",
      "*** epoch2,batch51 train loss 1.1534569263458252, dev loss 1.1408002376556396, dev f1 0.872102351729225, best f1-macro now:0.8725668663758895\n",
      "*** epoch2,batch101 train loss 1.09891939163208, dev loss 1.0507029294967651, dev f1 0.8778296147654233, best f1-macro now:0.8725668663758895\n",
      "*** epoch2,batch151 train loss 1.1170848608016968, dev loss 1.0804166793823242, dev f1 0.8781923529139736, best f1-macro now:0.8778296147654233\n",
      "*** epoch2,batch201 train loss 1.1419947147369385, dev loss 1.121132254600525, dev f1 0.8773903744058404, best f1-macro now:0.8781923529139736\n",
      "*** epoch2,batch251 train loss 1.1053757667541504, dev loss 1.0779087543487549, dev f1 0.8783756083589391, best f1-macro now:0.8781923529139736\n",
      "*** epoch2,batch301 train loss 1.0075442790985107, dev loss 1.0966575145721436, dev f1 0.8801074448465848, best f1-macro now:0.8783756083589391\n",
      "*** epoch3,batch1 train loss 1.1392742395401, dev loss 1.03428053855896, dev f1 0.8807748531839547, best f1-macro now:0.8801074448465848\n",
      "*** epoch3,batch51 train loss 1.1774910688400269, dev loss 1.1143683195114136, dev f1 0.879379221930624, best f1-macro now:0.8807748531839547\n",
      "*** epoch3,batch101 train loss 1.075524091720581, dev loss 1.0561943054199219, dev f1 0.8810606142646844, best f1-macro now:0.8807748531839547\n",
      "*** epoch3,batch151 train loss 1.0879977941513062, dev loss 1.1809511184692383, dev f1 0.8858603091421472, best f1-macro now:0.8810606142646844\n",
      "*** epoch3,batch201 train loss 1.1054471731185913, dev loss 1.1064488887786865, dev f1 0.8872036527620798, best f1-macro now:0.8858603091421472\n",
      "*** epoch3,batch251 train loss 1.0909395217895508, dev loss 1.1505848169326782, dev f1 0.8860497247071916, best f1-macro now:0.8872036527620798\n",
      "*** epoch3,batch301 train loss 1.075945496559143, dev loss 1.0564717054367065, dev f1 0.8881145154818295, best f1-macro now:0.8872036527620798\n",
      "*** epoch4,batch1 train loss 1.0522629022598267, dev loss 1.0739600658416748, dev f1 0.8868733141025938, best f1-macro now:0.8881145154818295\n",
      "*** epoch4,batch51 train loss 1.153597116470337, dev loss 1.1284176111221313, dev f1 0.8906992343972325, best f1-macro now:0.8881145154818295\n",
      "*** epoch4,batch101 train loss 1.1275732517242432, dev loss 1.0133000612258911, dev f1 0.8889750456896547, best f1-macro now:0.8906992343972325\n",
      "*** epoch4,batch151 train loss 1.0574672222137451, dev loss 1.121311068534851, dev f1 0.8920671639294238, best f1-macro now:0.8906992343972325\n",
      "*** epoch4,batch201 train loss 1.059450387954712, dev loss 1.1058191061019897, dev f1 0.8919273337279018, best f1-macro now:0.8920671639294238\n",
      "*** epoch4,batch251 train loss 1.06596040725708, dev loss 1.0862356424331665, dev f1 0.8924493367659133, best f1-macro now:0.8920671639294238\n",
      "*** epoch4,batch301 train loss 1.125512957572937, dev loss 1.136253833770752, dev f1 0.8952527867463707, best f1-macro now:0.8924493367659133\n",
      "*** epoch5,batch1 train loss 1.07391357421875, dev loss 1.0565944910049438, dev f1 0.895578812798006, best f1-macro now:0.8952527867463707\n",
      "*** epoch5,batch51 train loss 1.098044991493225, dev loss 1.032598853111267, dev f1 0.8911819853274443, best f1-macro now:0.895578812798006\n",
      "*** epoch5,batch101 train loss 1.075793743133545, dev loss 0.9933351278305054, dev f1 0.8901130385029159, best f1-macro now:0.895578812798006\n",
      "*** epoch5,batch151 train loss 1.0312016010284424, dev loss 1.0388998985290527, dev f1 0.8927236278029893, best f1-macro now:0.895578812798006\n",
      "*** epoch5,batch201 train loss 1.0494022369384766, dev loss 1.0947073698043823, dev f1 0.8961882987246348, best f1-macro now:0.895578812798006\n",
      "*** epoch5,batch251 train loss 1.0553903579711914, dev loss 1.0733319520950317, dev f1 0.890835397952463, best f1-macro now:0.8961882987246348\n",
      "*** epoch5,batch301 train loss 1.071089744567871, dev loss 1.1239184141159058, dev f1 0.8935884156695858, best f1-macro now:0.8961882987246348\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#训练\n",
    "\n",
    "#优化器定义\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)  \n",
    "#损失函数\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "#早停，如果10轮内指标不再上升就停止训练\n",
    "early_stop=10\n",
    "\n",
    "#模型保存路径\n",
    "save_path=f\"{BASE}/result/model.pt\"\n",
    "early_stop_flag = 0 \n",
    "\n",
    "#记录最好结果的模型epoch\n",
    "best_epoch = 0 \n",
    "best_batch = 0\n",
    "#最好的模型在验证集上的指标\n",
    "best_val_f_macro = 0\n",
    "\n",
    "#标识模型训练\n",
    "model.train()\n",
    "\n",
    "#训练epoch\n",
    "num_epoch=5\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    for batch_id, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(train_loader):\n",
    "        input_ids=input_ids.to(device);attention_mask=attention_mask.to(device);token_type_ids=token_type_ids.to(device)\n",
    "        labels=labels.to(device)\n",
    "        # print(input_ids.is_cuda)\n",
    "        out = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids)\n",
    "        #计算损失\n",
    "        train_loss = criterion(out, labels)\n",
    "        #反向传播\n",
    "        train_loss.backward()\n",
    "        #参数更新\n",
    "        optimizer.step()\n",
    "        #梯度清零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #每10batch计算验证集指标\n",
    "        if batch_id % 50 == 0:\n",
    "            #模型验证\n",
    "            model.eval()\n",
    "            #记录全部dev集的预测类别和标签类别\n",
    "            Labels=torch.tensor([]);Pres=torch.tensor([])\n",
    "            for i, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(dev_loader):\n",
    "                input_ids=input_ids.to(device);attention_mask=attention_mask.to(device);token_type_ids=token_type_ids.to(device)\n",
    "                out = model(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    token_type_ids=token_type_ids)\n",
    "                loss = criterion(out.cpu(), labels)\n",
    "                out = out.argmax(dim=1)\n",
    "                out.detach_() \n",
    "\n",
    "                #拼接\n",
    "                Labels=torch.concat([Labels,labels])\n",
    "                Pres=torch.concat([Pres,out.cpu()])\n",
    "            #指标计算\n",
    "            zb = model.compute(Labels, Pres)\n",
    "            # print(i,loss.item(), zb)\n",
    "            print(f\"*** epoch{epoch + 1},batch{batch_id+1} train loss {train_loss.item()}, dev loss {loss.item()}, dev f1 {zb['f1']}, best f1-macro now:{best_val_f_macro}\")      \n",
    "\n",
    "            #记录最优结果和保存模型\n",
    "            if zb['f1'] > best_val_f_macro:\n",
    "                best_val_f_macro = zb['f1']\n",
    "                best_epoch = epoch\n",
    "                best_batch = batch_id\n",
    "                early_stop_flag = 0\n",
    "                # 保存本轮训练结果\n",
    "                torch.save({'net':model.state_dict(), 'optimizer':optimizer.state_dict(), 'epoch':i, 'batch':batch_id}, save_path) \n",
    "            else:\n",
    "                early_stop_flag += 1\n",
    "                #早停\n",
    "                if early_stop_flag == early_stop:\n",
    "                    print(f'\\nThe model has not been improved for {early_stop} rounds. Stop early!')\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "275dd1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** test ***\n",
      "{'accuracy': 0.8890224358974359, 'f1': 0.8881352395536218, 'precision': 0.887867603173526, 'recall': 0.889002609200697}\n"
     ]
    }
   ],
   "source": [
    "#测试\n",
    "def test(model,test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_out=torch.tensor([]);test_label = torch.tensor([])\n",
    "    for epoch, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(test_loader):\n",
    "        input_ids=input_ids.to(device);attention_mask=attention_mask.to(device);token_type_ids=token_type_ids.to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        test_label=torch.concat([test_label,labels])\n",
    "        test_out=torch.concat([test_out,out.cpu()])\n",
    "\n",
    "    zb = model.compute(test_label, test_out)\n",
    "    print(f\"*** test ***\")     \n",
    "    print(zb)    \n",
    "\n",
    "#加载最优模型       \n",
    "test_model = Model(5,pretrained).to(device)\n",
    "test_model.load_state_dict(torch.load(save_path)['net'])\n",
    "test(test_model,test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('ccks': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3876d1b20a466b1321a8be942dd18371b99f27bece500b2313a9df5df869c7dd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
