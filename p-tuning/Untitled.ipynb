{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a3a012c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from vevestaX import vevesta as v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb4ba749",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--task_name TASK_NAME]\n",
      "                             [--data_path DATA_PATH] [--PLM_name PLM_NAME]\n",
      "                             [--print_num PRINT_NUM] [--eval_num EVAL_NUM]\n",
      "                             [--quick_exp_data_num QUICK_EXP_DATA_NUM]\n",
      "                             [--epoch EPOCH]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\npofsi\\AppData\\Roaming\\jupyter\\runtime\\kernel-583aa3fd-f1b7-452e-899e-d9772cdefdad.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\tools\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3465: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from dataloader import load_file\n",
    "from transformers import AutoModelForMaskedLM,AutoTokenizer\n",
    "import os\n",
    "from transformers.optimization import get_scheduler\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import argparse\n",
    "import sys\n",
    "from dataloader import SST2Dataset, SNLIDataset\n",
    "\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import os\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import sys\n",
    "from dataloader import load_file\n",
    "from dataloader import SST2Dataset, SNLIDataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def construct_generation_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--task_name\", type=str, default='SST-2')\n",
    "    parser.add_argument(\"--data_path\", type=str, default='.\\\\data')\n",
    "    parser.add_argument(\"--PLM_name\", type=str, default='bert-base-cased')\n",
    "    parser.add_argument(\"--print_num\", type=int, default=50)\n",
    "    parser.add_argument(\"--eval_num\", type=int, default=200)\n",
    "    parser.add_argument(\"--quick_exp_data_num\", type=int, default=10000)  \n",
    "    parser.add_argument(\"--epoch\", type=int, default=1)  \n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "args = construct_generation_args()\n",
    "\n",
    "def set_seed(seed=34):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "set_seed()\n",
    "\n",
    "model_name = args.PLM_name\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "pretrained_model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "\n",
    "task_name = args.task_name\n",
    "data_path = args.data_path\n",
    "\n",
    "if args.task_name == 'SST-2':\n",
    "    train_texts, train_labels = load_file(data_path,task_name,'train')\n",
    "    dev_texts, dev_labels = load_file(data_path,task_name,'dev')\n",
    "    if args.quick_exp_data_num is not None:\n",
    "        train_texts, train_labels = train_texts[:args.quick_exp_data_num], train_labels[:args.quick_exp_data_num]\n",
    "    train_set = SST2Dataset(args, train_texts, train_labels)\n",
    "    dev_set = SST2Dataset(args, dev_texts, dev_labels)\n",
    "elif args.task_name == 'SNLI':\n",
    "    train_texts_a, train_texts_b, train_labels = load_file(data_path,task_name,'train')\n",
    "    dev_texts_a, dev_texts_b, dev_labels = load_file(data_path,task_name,'dev')\n",
    "    if args.quick_exp_data_num is not None: \n",
    "        train_texts_a, train_texts_b, train_labels = train_texts_a[:args.quick_exp_data_num], train_texts_b[:args.quick_exp_data_num], train_labels[:args.quick_exp_data_num]\n",
    "    train_set = SNLIDataset(args, train_texts_a, train_texts_b, train_labels)\n",
    "    dev_set = SNLIDataset(args, dev_texts_a, dev_texts_b, dev_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True, drop_last=True)\n",
    "eval_dataloader = DataLoader(dev_set, batch_size=32, shuffle=False, drop_last=False)\n",
    " \n",
    "\n",
    "\n",
    "# tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
    "# tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "# tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "# small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "# small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "\n",
    "\n",
    "# train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
    "# eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "def get_embedding_layer(model):\n",
    "    embeddings = model.get_input_embeddings()\n",
    "    return embeddings\n",
    "\n",
    "def calculate_metrics(pred_ids, label_id):\n",
    "    metrics = {}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "class FineTuneForClassification(torch.nn.Module):\n",
    "    def __init__(self, args, device, pretrained_model, tokenizer):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.args = args\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = pretrained_model\n",
    "        self.model = self.model.to(self.device)\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = True\n",
    "        self.embeddings = get_embedding_layer(self.model)\n",
    "\n",
    "        # load prompt encoder\n",
    "        self.hidden_size = self.embeddings.embedding_dim\n",
    "        # self.tokenizer.add_special_tokens({'additional_special_tokens': ['[PROMPT]']})\n",
    "        # self.pseudo_token_id = self.tokenizer.get_vocab()['[PROMPT]']\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else self.tokenizer.unk_token_id\n",
    "        # self.prompt_encoder = PromptEncoderPrefixLSTM(self.hidden_size, self.tokenizer, self.device)\n",
    "        # self.prompt_encoder = self.prompt_encoder.to(self.device)\n",
    "        self.ce_loss = torch.nn.CrossEntropyLoss()\n",
    "        if args.task_name == 'SST-2':\n",
    "            self.label_map = {0:'bad',1:'great'}\n",
    "        elif args.task_name == 'SNLI':\n",
    "            self.label_map = {0: 'No', 1: 'Maybe', 2:'Yes'}\n",
    "\n",
    "\n",
    "\n",
    "    def tokenize(self, query, tokens=0):\n",
    "        token_ids = self.tokenizer.encode(''+query, add_special_tokens=True)\n",
    "        return token_ids\n",
    "\n",
    "    def get_query(self, sentence,tokens=0, sentence2=None):\n",
    "        if self.args.task_name == 'SST-2':\n",
    "            query = f'{sentence}.'# It is {self.tokenizer.mask_token} \n",
    "        elif self.args.task_name == 'SNLI':\n",
    "            query = f'{sentence}, {sentence2}'#{self.tokenizer.mask_token}\n",
    "        #query_Lstr=self.tokenizer.tokenize(query) \n",
    "        #print(self.tokenizer.encode(query_Lstr, add_special_tokens=True))\n",
    "        #print(query)\n",
    "        \n",
    "        return tokenizer.encode_plus(\n",
    "                query,                  # Sentence to encode.\n",
    "                add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "                padding='max_length',\n",
    "                max_length = 45,           # Pad & truncate all sentences.\n",
    "                pad_to_max_length=True,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,   # Construct attn. masks.\n",
    "                return_tensors='pt',     # Return pytorch tensors.\n",
    "            )#self.tokenize(query, tokens)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "    def evalcase(self,query):\n",
    "        self.eval()\n",
    "        edict=tokenizer.encode_plus(\n",
    "                query,                  # Sentence to encode.\n",
    "                add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "                padding='max_length',\n",
    "                max_length = 45,           # Pad & truncate all sentences.\n",
    "                pad_to_max_length=True,\n",
    "                truncation=True,\n",
    "                return_attention_mask=True,   # Construct attn. masks.\n",
    "                return_tensors='pt',     # Return pytorch tensors.\n",
    "            )\n",
    "        query_output = self.model(**edict)\n",
    "        \n",
    "        logits = query_output['logits']\n",
    "        if self.args.task_name == 'SST-2':\n",
    "            interested_logits = logits[:, :,\n",
    "                                       [self.tokenizer.convert_tokens_to_ids('bad'),\n",
    "                                        self.tokenizer.convert_tokens_to_ids('great')]]\n",
    "        elif self.args.task_name == 'SNLI':\n",
    "            interested_logits = logits[:, :,\n",
    "                                       [self.tokenizer.convert_tokens_to_ids('Yes'),\n",
    "                                        self.tokenizer.convert_tokens_to_ids('Maybe'),\n",
    "                                        self.tokenizer.convert_tokens_to_ids('No')]]\n",
    "        \n",
    "        pred_ids = torch.argsort(interested_logits, dim=2, descending=True)\n",
    "        batch_interested_logits = []\n",
    "        predicted_label = [] \n",
    "        \n",
    "        \n",
    "        for i in range(bz):\n",
    "            pred_seq = pred_ids[i, 0].tolist()\n",
    "            predicted_label.append(pred_seq[0])\n",
    "            batch_interested_logits.append(\n",
    "                interested_logits[i, 0].cpu())\n",
    "\n",
    "        return pred_ids[0]\n",
    "\n",
    "    def forward(self, sentences, labels, sentences2=None,epoch_i=-1,batch_i=-1): \n",
    "        #self.model.zero_grad()  \n",
    "        bz = len(sentences)\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        queries = []\n",
    "        # For every sentence...\n",
    "        for sent_idx, sent in enumerate(sentences):\n",
    "\n",
    "            if sentences2 is not None:\n",
    "                encoded_dict = self.get_query(sent,0,sentences2[sent_idx])\n",
    "            else:\n",
    "                encoded_dict = self.get_query(sent, 0)\n",
    "            # 将编码后的文本加入到列表  \n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            \n",
    "            # 将文本的 attention mask 也加入到 attention_masks 列表\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "        # 将列表转换为 tensor\n",
    "        input_ids = torch.cat(input_ids, dim=0)\n",
    "        attention_masks = torch.cat(attention_masks, dim=0)\n",
    "        labels = labels.clone().detach()\n",
    "\n",
    "        # 输出第 1 行文本的原始和编码后的信息\n",
    "        # print('Original: ', sentences[0])\n",
    "        # print('Token IDs:', input_ids[0])\n",
    "\n",
    "            # if self.args.task_name == 'SST-2':\n",
    "            #     queries = [torch.LongTensor(self.get_query(\n",
    "            #         sentences[i],0)).squeeze(0) for i in range(bz)]\n",
    "            # elif self.args.task_name == 'SNLI':\n",
    "            #     queries = [torch.LongTensor(self.get_query(\n",
    "            #         sentences[i],0,sentences2[i])).squeeze(0) for i in range(bz)]\n",
    "        \n",
    "        # print((queries))\n",
    "        \n",
    "        #queries = self.tokenizer.encode_plus()\n",
    "        #queries = pad_sequence(\n",
    "        #   queries, True, padding_value=self.pad_token_id,).long().to(self.device)\n",
    "        #print(\"[===========]\\n{}:===={}\",queries.shape)\n",
    "        label_ids = torch.LongTensor(labels).reshape((bz, -1)).to(self.device)\n",
    "        attention_mask = queries != self.pad_token_id\n",
    "        #inputs_embeds = self.embed_input(queries)\n",
    "      \n",
    "        #label_mask = (queries == self.tokenizer.mask_token_id).nonzero().reshape(bz, -1)[:, 1].unsqueeze(1).to(self.device)  # bz * 1\n",
    "        query_output = self.model(torch.Tensor(input_ids).to(device),\n",
    "                            attention_mask=attention_masks.to(self.device).bool(),\n",
    "                            output_hidden_states=True,\n",
    "                            return_dict=True)\n",
    "        \n",
    "        logits = query_output['logits']\n",
    "        if self.args.task_name == 'SST-2':\n",
    "            interested_logits = logits[:, :,\n",
    "                                       [self.tokenizer.convert_tokens_to_ids('bad'),\n",
    "                                        self.tokenizer.convert_tokens_to_ids('great')]]\n",
    "        elif self.args.task_name == 'SNLI':\n",
    "            interested_logits = logits[:, :,\n",
    "                                       [self.tokenizer.convert_tokens_to_ids('Yes'),\n",
    "                                        self.tokenizer.convert_tokens_to_ids('Maybe'),\n",
    "                                        self.tokenizer.convert_tokens_to_ids('No')]]\n",
    "        \n",
    "        pred_ids = torch.argsort(interested_logits, dim=2, descending=True)\n",
    "        batch_interested_logits = []\n",
    "        predicted_label = [] \n",
    "        \n",
    "        \n",
    "        for i in range(bz):\n",
    "            pred_seq = pred_ids[i, 0].tolist()\n",
    "            predicted_label.append(pred_seq[0])\n",
    "            batch_interested_logits.append(\n",
    "                interested_logits[i, 0].cpu())\n",
    "\n",
    "        \n",
    "        batch_interested_logits = torch.stack(\n",
    "            batch_interested_logits).to(self.device)\n",
    "        #print(batch_interested_logits.shape)\n",
    "        if epoch_i!=-1 and batch_i!=-1 and epoch_i == batch_i:\n",
    "            print(\"Save log\")\n",
    "            lines=[]\n",
    "            \n",
    "            for i,d in enumerate(sentences):\n",
    "                indices = torch.LongTensor([i, 1, 1]).cpu()\n",
    "                batch_interested_logits_c=batch_interested_logits.clone().detach().cpu()\n",
    "                t=torch.index_select(batch_interested_logits_c, 0, indices)\n",
    "                if sentences2 is not None:\n",
    "                    lines.append(\"\"+str(predicted_label[i])+\"|\"+str(labels[i])+\", \"+str(sentences[i])+\" + \"+str(sentences2[i])+\", \"+str(t)+\"\\n\")\n",
    "                else:\n",
    "                    lines.append(\"\"+str(predicted_label[i])+\"|\"+str(labels[i])+\", \"+str(sentences[i])+\", \"+str(t)+\"\\n\")\n",
    "\n",
    "\n",
    "            fo = open(\"log\\\\\"+args.task_name+\"_batchlog_\"+str(epoch_i)+\"_\"+str(batch_i)+\".txt\", mode=\"w\", encoding=\"utf-8\")\n",
    "            fo.writelines(lines)\n",
    "\n",
    "        loss = self.ce_loss(batch_interested_logits, label_ids.squeeze(1))\n",
    "        return loss, predicted_label\n",
    "\n",
    "\n",
    "def train():\n",
    "    num_training_steps = args.epoch * len(train_dataloader)\n",
    "    model = FineTuneForClassification(args,device,pretrained_model,tokenizer)\n",
    "    optimizer = AdamW(model.model.parameters(), lr=5e-5)\n",
    "    my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "        optimizer=optimizer, gamma=0.98)\n",
    "    #print(len(train_dataloader))\n",
    "    perf = 0\n",
    "    best_dev = 0.0\n",
    "    early_stop = 20\n",
    "    \n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch_idx in tqdm(range(args.epoch)):\n",
    "        total_train_pred = []\n",
    "        total_train_labels = []\n",
    "        for batch_idx, batch_data in enumerate(tqdm(train_dataloader)):\n",
    "            model.train()\n",
    "            if args.task_name == 'SST-2':\n",
    "                sentences, labels = batch_data\n",
    "                loss, pred = model(sentences, labels)\n",
    "            elif args.task_name == 'SNLI':\n",
    "                sentences_a, sentences_b, labels = batch_data\n",
    "                loss, pred = model(sentences_a, labels, sentences_b)\n",
    "            #loss.requires_grad = True\n",
    "            total_train_pred += pred\n",
    "            total_train_labels += labels.tolist()\n",
    "            \n",
    "            if batch_idx % args.print_num == 0 and batch_idx > 0:\n",
    "                acc = (torch.tensor(total_train_labels).long() == torch.tensor(\n",
    "                    total_train_pred).long()).sum() / len(total_train_labels)\n",
    "                print(f'train_loss: {loss.item()}, train_acc: {acc}')\n",
    "                total_train_pred = []\n",
    "                total_train_labels = []\n",
    "\n",
    "            # with torch.no_grad():\n",
    "                \n",
    "                # total_dev_pred = []\n",
    "                # total_dev_labels = []\n",
    "            #torch.set_grad_enabled(True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            # my_lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "\n",
    "            if batch_idx % args.eval_num == 0 and batch_idx > 0:\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    total_dev_loss = 0\n",
    "                    total_dev_pred = []\n",
    "                    total_dev_labels = []\n",
    "\n",
    "                    if args.task_name == 'SST-2':\n",
    "                        for batch_idx, batch_data in tqdm(enumerate(eval_dataloader)):\n",
    "                            sentences, labels = batch_data\n",
    "                            dev_loss, dev_pred = model(sentences, labels, epoch_i=epoch_idx, batch_i=batch_idx)\n",
    "                            total_dev_loss += dev_loss.item()\n",
    "                            total_dev_pred += dev_pred\n",
    "                            total_dev_labels += labels.tolist()\n",
    "                    elif args.task_name == 'SNLI':\n",
    "                        for batch_idx, batch_data in tqdm(enumerate(eval_dataloader)):\n",
    "                            sentences_a, sentences_b, labels = batch_data\n",
    "                            dev_loss, dev_pred = model(\n",
    "                                sentences_a, labels, sentences_b, epoch_i=epoch_idx, batch_i=batch_idx)\n",
    "                            total_dev_loss += dev_loss.item()\n",
    "                            total_dev_pred += dev_pred\n",
    "                            total_dev_labels += labels.tolist()\n",
    "\n",
    "                dev_acc = (torch.tensor(total_dev_labels).long() == torch.tensor(\n",
    "                    total_dev_pred).long()).sum() / len(total_dev_labels)\n",
    "                total_dev_loss = total_dev_loss/(batch_idx+1)\n",
    "                print(f'dev_loss: {total_dev_loss}, dev_acc: {dev_acc}')\n",
    "                if dev_acc > best_dev:\n",
    "                    best_dev = dev_acc\n",
    "                    best_ckpt = {\n",
    "                        'embedding': 0, 'best_dev': best_dev}\n",
    "                    if not os.path.exists('./saved_modelf_{}/'.format(args.task_name)):\n",
    "                        os.makedirs('./saved_modelf_{}/'.format(args.task_name))\n",
    "                    torch.save(best_ckpt, './saved_modelf_{}/{}_{}.pt'.format(\n",
    "                        args.task_name, epoch_idx, str(round(float(best_dev), 4))))\n",
    "        my_lr_scheduler.step()\n",
    "        return model\n",
    "\n",
    "model=train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "baae6bc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13560\\1683357341.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLimeTextExplainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m83\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mexp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexplain_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sdd fdijfw wes\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.54\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_in_notebook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\lime\\lime_text.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[1;34m(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[0;32m    411\u001b[0m                                         mask_string=self.mask_string))\n\u001b[0;32m    412\u001b[0m         \u001b[0mdomain_mapper\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextDomainMapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexed_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         data, yss, distances = self.__data_labels_distances(\n\u001b[0m\u001b[0;32m    414\u001b[0m             \u001b[0mindexed_string\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m             distance_metric=distance_metric)\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\lime\\lime_text.py\u001b[0m in \u001b[0;36m__data_labels_distances\u001b[1;34m(self, indexed_string, classifier_fn, num_samples, distance_metric)\u001b[0m\n\u001b[0;32m    480\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minactive\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m             \u001b[0minverse_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexed_string\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_removing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minactive\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minverse_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistance_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "class_names=[\"yes\",\"no\"]\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "idx = 83\n",
    "exp = explainer.explain_instance(\"sdd fdijfw wes\", [0.54], num_features=6)\n",
    "exp.as_list()\n",
    "exp.show_in_notebook(text=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7844be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb20466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
